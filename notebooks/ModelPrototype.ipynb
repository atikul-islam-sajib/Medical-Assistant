{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d2a028",
   "metadata": {},
   "source": [
    "## Import the Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08855055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970568f",
   "metadata": {},
   "source": [
    "## Utility Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_files():\n",
    "    with open(\"../../notebook_config.yml\", mode=\"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def dump_files(value=None, filename=None):\n",
    "    if (value is None) and (filename is None):\n",
    "        raise ValueError(\"Either values or filename must be provided\".capitalize())\n",
    "    else:\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "\n",
    "def load_files(filename: str = None):\n",
    "    if filename is None:\n",
    "        raise ValueError(\"Filename must be provided\".capitalize())\n",
    "    else:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "\n",
    "def device_init(device: str = \"cuda\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc84b",
   "metadata": {},
   "source": [
    "## DataLoader (Yet to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str = \"./data/raw\",\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        batch_size: int = 64,\n",
    "        split_size: float = 0.25,\n",
    "    ):\n",
    "        self.image_path = image_path\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "        self.valid_images = []\n",
    "        self.valid_labels = []\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if not os.path.exists(\"./data/processed\"):\n",
    "            os.makedirs(\"./data/processed\")\n",
    "\n",
    "        with zipfile.ZipFile(file=self.image_path, mode=\"r\") as file:\n",
    "            file.extractall(path=\"./data/processed\")\n",
    "\n",
    "        print(\"\"\"Extracted file saved in the \"./data/processed\" folder\"\"\")\n",
    "\n",
    "    def split_dataset(self, **kwargs):\n",
    "        X = kwargs[\"X\"]\n",
    "        y = kwargs[\"y\"]\n",
    "\n",
    "        if not isinstance(X, list) and not isinstance(y, list):\n",
    "            raise ValueError(\"Invalid data type\".capitalize())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.split_size, random_state=42\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "    def image_transforms(self, type: str = \"RGB\"):\n",
    "        if type == \"RGB\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize([0.5], [0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def extract_features(self):\n",
    "        train_path = \"./data/processed/Training\"\n",
    "        valid_path = \"./data/processed/Testing\"\n",
    "\n",
    "        class_names = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "\n",
    "        for path in [train_path, valid_path]:\n",
    "            for label in class_names:\n",
    "                image_path = os.path.join(path, label)\n",
    "                for image in tqdm(\n",
    "                    os.listdir(image_path), desc=\"Extracting images\".title()\n",
    "                ):\n",
    "                    single_image_path = os.path.join(image_path, image)\n",
    "                    if not single_image_path.endswith((\"png\", \"jpg\", \"jpeg\")):\n",
    "                        raise ValueError(\"Invalid image format\")\n",
    "\n",
    "                    image = cv2.imread(single_image_path)\n",
    "                    image = Image.fromarray(image)\n",
    "                    image = self.image_transforms(\n",
    "                        \"GRAY\" if self.image_channels == 1 else \"RGB\"\n",
    "                    )(image)\n",
    "\n",
    "                    if path == train_path:\n",
    "                        self.train_images.append(image)\n",
    "                        self.train_labels.append(class_names.index(label))\n",
    "                    else:\n",
    "                        self.valid_images.append(image)\n",
    "                        self.valid_labels.append(class_names.index(label))\n",
    "\n",
    "        assert len(self.train_images) == len(self.train_labels)\n",
    "        assert len(self.valid_images) == len(self.valid_labels)\n",
    "\n",
    "        train_dataset = self.split_dataset(X=self.train_images, y=self.train_labels)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": torch.stack(train_dataset[\"X_train\"][:400]).float(),\n",
    "            \"X_test\": torch.stack(train_dataset[\"X_test\"][:400]).float(),\n",
    "            \"y_train\": torch.tensor(train_dataset[\"y_train\"][:100], dtype=torch.long),\n",
    "            \"y_test\": torch.tensor(train_dataset[\"y_test\"][:100], dtype=torch.long),\n",
    "            \"valid_images\": torch.stack(self.valid_images[:50]).float(),\n",
    "            \"valid_labels\": torch.tensor(self.valid_labels[:50], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        dataset = self.extract_features()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_train\"], dataset[\"y_train\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_test\"], dataset[\"y_test\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"valid_images\"], dataset[\"valid_labels\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for value, filename in tqdm(\n",
    "            [\n",
    "                (train_dataloader, \"train_dataloader.pkl\"),\n",
    "                (test_dataloader, \"test_dataloader.pkl\"),\n",
    "                (valid_dataloader, \"valid_dataloader.pkl\"),\n",
    "            ],\n",
    "            desc=\"Saving dataloaders\".title(),\n",
    "        ):\n",
    "            dump_files(\n",
    "                value=value, filename=os.path.join(\"./data/processed/\", filename)\n",
    "            )\n",
    "\n",
    "        print(\"Files saved in the folder ./data/processed/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Dataloader for the Medical Assistant Task\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_path\",\n",
    "        type=str,\n",
    "        default=\"./data/raw/dataset.zip\",\n",
    "        help=\"Path to the dataset\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_channels\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of image channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_size\", type=int, default=224, help=\"Image size\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=16, help=\"Batch size\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_size\", type=float, default=0.30, help=\"Split size\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    image_path = args.image_path\n",
    "    image_channels = args.image_channels\n",
    "    image_size = args.image_size\n",
    "    batch_size = args.batch_size\n",
    "    split_size = args.split_size\n",
    "\n",
    "    loader = Loader(\n",
    "        image_path=image_path,\n",
    "        image_channels=image_channels,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        split_size=split_size,\n",
    "    )\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2967ebf",
   "metadata": {},
   "source": [
    "## Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dimension: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dimension = dimension\n",
    "\n",
    "        self.encoding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                size=(\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                ),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            return x + self.encoding\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        embedding_dimension: int = 512,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.total_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        if self.embedding_dimension is None:\n",
    "            warnings.warn(\n",
    "                \"Embedding dimension not specified. Using the default value calculated as: image_channels × patch_size × patch_size.\"\n",
    "            )\n",
    "            self.embedding_dimension = (\n",
    "                self.image_channels**self.patch_size * self.patch_size\n",
    "            )\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=self.image_channels,\n",
    "            out_channels=self.embedding_dimension,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=self.patch_size // self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.encoding = PositionalEncoding(dimension=self.embedding_dimension)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.projection(x)\n",
    "            x = x.view(x.size(0), x.size(-1) * x.size(-2), x.size(1))\n",
    "            x = self.encoding(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 3\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    embedding_dimension = 768\n",
    "\n",
    "    patchEmbedding = PatchEmbedding(\n",
    "        image_channels=image_channels,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((64, 3, 224, 224))\n",
    "\n",
    "    assert patchEmbedding(images).size() == (\n",
    "        64,\n",
    "        (image_size // patch_size) ** 2,\n",
    "        768,\n",
    "    ), \"Patch Embedding is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a57450",
   "metadata": {},
   "source": [
    "## Multi Head Attention Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b66be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "    if (\n",
    "        not isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(value, torch.Tensor)\n",
    "    ):\n",
    "        raise TypeError(\"All inputs must be torch.Tensor\".capitalize())\n",
    "\n",
    "    key = key.transpose(-2, -1)\n",
    "    scores = torch.matmul(query, key) / math.sqrt(key.size(-1))\n",
    "    scores = torch.softmax(scores, dim=-1)\n",
    "    attention = torch.matmul(scores, value)\n",
    "    return attention\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 1\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    embedding_dimension = image_channels * patch_size * patch_size\n",
    "\n",
    "    attention = scaled_dot_product(\n",
    "        query=torch.randn(1, total_patches, embedding_dimension),\n",
    "        key=torch.randn(1, total_patches, embedding_dimension),\n",
    "        value=torch.randn(1, total_patches, embedding_dimension),\n",
    "    )\n",
    "\n",
    "    assert attention.size() == (\n",
    "        total_patches // total_patches,\n",
    "        total_patches,\n",
    "        embedding_dimension,\n",
    "    ), \"Attention output size must match the input size\".capitalize()\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, nheads: int = 6, dimension: int = 768):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dimension = dimension\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.nheads == 0\n",
    "        ), \"Dimension must be divisible by number of heads\".capitalize()\n",
    "\n",
    "        warnings.warn(\n",
    "            \"Invalid number of dimensions provided. To avoid errors, ensure the dimension is calculated as: in_channels × patch_size × patch_size.\"\n",
    "        )\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "        else:\n",
    "            QKV = self.QKV(x)\n",
    "            query, key, value = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "            assert (\n",
    "                query.size() == key.size() == value.size()\n",
    "            ), \"Query, key, and value must have the same size\".capitalize()\n",
    "\n",
    "            query = query.view(\n",
    "                query.size(0), query.size(1), self.nheads, query.size(-1) // self.nheads\n",
    "            )\n",
    "            key = key.view(\n",
    "                key.size(0), key.size(1), self.nheads, key.size(-1) // self.nheads\n",
    "            )\n",
    "            value = value.view(\n",
    "                value.size(0), value.size(1), self.nheads, value.size(-1) // self.nheads\n",
    "            )\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "            attention = scaled_dot_product(query=query, key=key, value=value)\n",
    "            attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(-2),\n",
    "                attention.size(1),\n",
    "                attention.size(-1),\n",
    "            )\n",
    "            attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(1),\n",
    "                attention.size(2) * attention.size(3),\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                attention.size() == x.size()\n",
    "            ), \"Attention output must have the same size as input\".capitalize()\n",
    "\n",
    "            return attention\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nheads = 8\n",
    "    dimension = 256\n",
    "\n",
    "    images = torch.randn((1, 196, 256))\n",
    "    multihead_attention = MultiHeadAttentionLayer(nheads=nheads, dimension=dimension)\n",
    "\n",
    "    assert (\n",
    "        multihead_attention(x=images)\n",
    "    ).size() == images.size(), \"MultiHeadAttention is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fddf55",
   "metadata": {},
   "source": [
    "## Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape: int = 768, eps=1e-05):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.dimension = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(\n",
    "            data=torch.ones(\n",
    "                (\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                )\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.beta = nn.Parameter(\n",
    "            data=torch.zeros(\n",
    "                (\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                )\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "        else:\n",
    "            mean = torch.mean(x, dim=-1)\n",
    "            variance = torch.var(x, dim=-1)\n",
    "\n",
    "            mean = mean.unsqueeze(-1)\n",
    "            variance = variance.unsqueeze(-1)\n",
    "\n",
    "            x_bar = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "\n",
    "            return self.alpha * x_bar + self.beta\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 3\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    \n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    dimension = (image_channels * patch_size * patch_size)\n",
    "    \n",
    "    norm = LayerNormalization(normalized_shape=dimension)\n",
    "    images = torch.randn((image_channels//image_channels, total_patches, dimension))\n",
    "\n",
    "    assert (\n",
    "        norm(images).size()\n",
    "    ) == images.size(), \"Layer Normalization is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4149f",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation_func = activation\n",
    "\n",
    "        self.in_features = self.d_model\n",
    "        self.out_features = self.dim_feedforward\n",
    "\n",
    "        if self.activation_func == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif self.activation_func == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        elif self.activation_func == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\".capitalize())\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers += [\n",
    "                nn.Linear(\n",
    "                    in_features=self.in_features,\n",
    "                    out_features=self.out_features,\n",
    "                    bias=False,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers += [self.activation]\n",
    "                self.layers += [nn.Dropout(p=self.dropout)]\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = self.d_model\n",
    "\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.network(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    image_channels = 1\n",
    "    dropout = 0.1\n",
    "\n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    dimension = image_channels * patch_size**2\n",
    "    dim_feedforward = 4 * dimension\n",
    "\n",
    "    images = torch.randn(image_channels // image_channels, total_patches, dimension)\n",
    "\n",
    "    network = FeedForwardNeuralNetwork(\n",
    "        d_model=dimension,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    assert (\n",
    "        network(images).size()\n",
    "    ) == images.size(), \"FFNN is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628ee10",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nhead: int = 8,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.nheads = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttentionLayer(\n",
    "            nheads=self.nheads,\n",
    "            dimension=self.d_model,\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNormalization(\n",
    "            normalized_shape=self.d_model, eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.layer_norm2 = LayerNormalization(\n",
    "            normalized_shape=self.d_model, eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward_network = FeedForwardNeuralNetwork(\n",
    "            d_model=self.d_model,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            residual = x\n",
    "            \n",
    "            x = self.multi_head_attention(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm1(x)\n",
    "            \n",
    "            residual = x\n",
    "            \n",
    "            x = self.feed_forward_network(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm2(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformer = TransformerEncoderBlock(\n",
    "        nhead=8,\n",
    "        d_model=256,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "    \n",
    "    images = torch.randn((1, 196, 256))\n",
    "    \n",
    "    assert (transformer(images).size()) == images.size(), \"Transformer is not working properly\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73328283",
   "metadata": {},
   "source": [
    "## ViT - With Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, dimension: int = 768, dropout: float = 0.3, activation: str = \"leaky\"\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.dropout = dropout\n",
    "        self.activation_func = activation\n",
    "\n",
    "        if self.activation_func == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif self.activation_func == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(inplace=True)\n",
    "        elif self.activation_func == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif self.activation_func == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.in_features = self.dimension\n",
    "        self.out_features = self.in_features // 4\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers += [\n",
    "                nn.Linear(in_features=self.in_features, out_features=self.out_features)\n",
    "            ]\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers += [nn.BatchNorm1d(num_features=self.out_features)]\n",
    "                self.layers += [self.activation]\n",
    "                self.layers += [nn.Dropout(p=self.dropout)]\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = self.out_features // 4\n",
    "\n",
    "        self.layers += [\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=self.in_features, out_features=4),\n",
    "                nn.Softmax(dim=1),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.classifier = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            return self.classifier(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = Classifier(\n",
    "        dimension=256,\n",
    "        dropout=0.3,\n",
    "        activation=\"leaky\",\n",
    "    )\n",
    "\n",
    "    images = torch.randn((16, 196, 256))\n",
    "    images = torch.mean(images, dim=1)\n",
    "    \n",
    "    assert (classifier(images).size()) == (16, 4), \"Classifier is not working properly\".capitalize()\n",
    "\n",
    "class ViTWithClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        target_size: int = 4,\n",
    "        encoder_layer: int = 4,\n",
    "        nhead: int = 8,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(ViTWithClassifier, self).__init__()\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.target_size = target_size\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_channels=self.image_channels,\n",
    "            image_size=self.image_size,\n",
    "            patch_size=self.patch_size,\n",
    "            embedding_dimension=self.d_model,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(\n",
    "                    nhead=self.nhead,\n",
    "                    d_model=self.d_model,\n",
    "                    dim_feedforward=self.dim_feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation,\n",
    "                    layer_norm_eps=self.layer_norm_eps,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in tqdm(\n",
    "                    range(self.encoder_layer), desc=\"Transformer Block\".title()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = Classifier(\n",
    "            dimension=self.d_model,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.patch_embedding(x)\n",
    "\n",
    "            for layer in self.transformer:\n",
    "                x = layer(x)\n",
    "\n",
    "            x = torch.mean(x, dim=1)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vit = ViTWithClassifier(\n",
    "        image_channels=1,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        target_size=4,\n",
    "        encoder_layer=4,\n",
    "        nhead=8,\n",
    "        d_model=256,\n",
    "        dim_feedforward=4 * 256,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((16, 1, 224, 224))\n",
    "\n",
    "    assert (vit(images).size()) == (16, 4), \"ViTWithClassifier is not working properly\".capitalize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

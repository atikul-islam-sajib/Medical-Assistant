{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2d2a028",
   "metadata": {},
   "source": [
    "## Import the Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08855055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import zipfile\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from dotenv import load_dotenv\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sklearn.model_selection import train_test_split\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970568f",
   "metadata": {},
   "source": [
    "## Utility Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fe30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def config_files():\n",
    "    with open(\"../../notebook_config.yml\", mode=\"r\") as file:\n",
    "        return yaml.safe_load(file)\n",
    "\n",
    "\n",
    "def dump_files(value=None, filename=None):\n",
    "    if (value is None) and (filename is None):\n",
    "        raise ValueError(\"Either values or filename must be provided\".capitalize())\n",
    "    else:\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "\n",
    "def load_files(filename: str = None):\n",
    "    if filename is None:\n",
    "        raise ValueError(\"Filename must be provided\".capitalize())\n",
    "    else:\n",
    "        return joblib.load(filename=filename)\n",
    "\n",
    "\n",
    "def device_init(device: str = \"cuda\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def weight_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ccc84b",
   "metadata": {},
   "source": [
    "## DataLoader (Yet to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_path: str = \"./data/raw\",\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        batch_size: int = 64,\n",
    "        split_size: float = 0.25,\n",
    "    ):\n",
    "        self.image_path = image_path\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.split_size = split_size\n",
    "\n",
    "        self.train_images = []\n",
    "        self.train_labels = []\n",
    "        self.valid_images = []\n",
    "        self.valid_labels = []\n",
    "\n",
    "    def unzip_folder(self):\n",
    "        if not os.path.exists(\"./data/processed\"):\n",
    "            os.makedirs(\"./data/processed\")\n",
    "\n",
    "        with zipfile.ZipFile(file=self.image_path, mode=\"r\") as file:\n",
    "            file.extractall(path=\"./data/processed\")\n",
    "\n",
    "        print(\"\"\"Extracted file saved in the \"./data/processed\" folder\"\"\")\n",
    "\n",
    "    def split_dataset(self, **kwargs):\n",
    "        X = kwargs[\"X\"]\n",
    "        y = kwargs[\"y\"]\n",
    "\n",
    "        if not isinstance(X, list) and not isinstance(y, list):\n",
    "            raise ValueError(\"Invalid data type\".capitalize())\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=self.split_size, random_state=42\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"X_train\": X_train,\n",
    "            \"X_test\": X_test,\n",
    "            \"y_train\": y_train,\n",
    "            \"y_test\": y_test,\n",
    "        }\n",
    "\n",
    "    def image_transforms(self, type: str = \"RGB\"):\n",
    "        if type == \"RGB\":\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            return transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((self.image_size, self.image_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.CenterCrop((self.image_size, self.image_size)),\n",
    "                    transforms.Normalize([0.5], [0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def extract_features(self):\n",
    "        train_path = \"./data/processed/Training\"\n",
    "        valid_path = \"./data/processed/Testing\"\n",
    "\n",
    "        class_names = [\"glioma\", \"meningioma\", \"pituitary\", \"notumor\"]\n",
    "\n",
    "        for path in [train_path, valid_path]:\n",
    "            for label in class_names:\n",
    "                image_path = os.path.join(path, label)\n",
    "                for image in tqdm(\n",
    "                    os.listdir(image_path), desc=\"Extracting images\".title()\n",
    "                ):\n",
    "                    single_image_path = os.path.join(image_path, image)\n",
    "                    if not single_image_path.endswith((\"png\", \"jpg\", \"jpeg\")):\n",
    "                        raise ValueError(\"Invalid image format\")\n",
    "\n",
    "                    image = cv2.imread(single_image_path)\n",
    "                    image = Image.fromarray(image)\n",
    "                    image = self.image_transforms(\n",
    "                        \"GRAY\" if self.image_channels == 1 else \"RGB\"\n",
    "                    )(image)\n",
    "\n",
    "                    if path == train_path:\n",
    "                        self.train_images.append(image)\n",
    "                        self.train_labels.append(class_names.index(label))\n",
    "                    else:\n",
    "                        self.valid_images.append(image)\n",
    "                        self.valid_labels.append(class_names.index(label))\n",
    "\n",
    "        assert len(self.train_images) == len(self.train_labels)\n",
    "        assert len(self.valid_images) == len(self.valid_labels)\n",
    "\n",
    "        train_dataset = self.split_dataset(X=self.train_images, y=self.train_labels)\n",
    "\n",
    "        return {\n",
    "            \"X_train\": torch.stack(train_dataset[\"X_train\"][:400]).float(),\n",
    "            \"X_test\": torch.stack(train_dataset[\"X_test\"][:400]).float(),\n",
    "            \"y_train\": torch.tensor(train_dataset[\"y_train\"][:100], dtype=torch.long),\n",
    "            \"y_test\": torch.tensor(train_dataset[\"y_test\"][:100], dtype=torch.long),\n",
    "            \"valid_images\": torch.stack(self.valid_images[:50]).float(),\n",
    "            \"valid_labels\": torch.tensor(self.valid_labels[:50], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        dataset = self.extract_features()\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_train\"], dataset[\"y_train\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        test_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"X_test\"], dataset[\"y_test\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        valid_dataloader = DataLoader(\n",
    "            dataset=list(zip(dataset[\"valid_images\"], dataset[\"valid_labels\"])),\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        for value, filename in tqdm(\n",
    "            [\n",
    "                (train_dataloader, \"train_dataloader.pkl\"),\n",
    "                (test_dataloader, \"test_dataloader.pkl\"),\n",
    "                (valid_dataloader, \"valid_dataloader.pkl\"),\n",
    "            ],\n",
    "            desc=\"Saving dataloaders\".title(),\n",
    "        ):\n",
    "            dump_files(\n",
    "                value=value, filename=os.path.join(\"./data/processed/\", filename)\n",
    "            )\n",
    "\n",
    "        print(\"Files saved in the folder ./data/processed/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Dataloader for the Medical Assistant Task\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_path\",\n",
    "        type=str,\n",
    "        default=\"./data/raw/dataset.zip\",\n",
    "        help=\"Path to the dataset\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_channels\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of image channels\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--image_size\", type=int, default=224, help=\"Image size\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=16, help=\"Batch size\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_size\", type=float, default=0.30, help=\"Split size\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    image_path = args.image_path\n",
    "    image_channels = args.image_channels\n",
    "    image_size = args.image_size\n",
    "    batch_size = args.batch_size\n",
    "    split_size = args.split_size\n",
    "\n",
    "    loader = Loader(\n",
    "        image_path=image_path,\n",
    "        image_channels=image_channels,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        split_size=split_size,\n",
    "    )\n",
    "\n",
    "    loader.unzip_folder()\n",
    "    loader.create_dataloader()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2967ebf",
   "metadata": {},
   "source": [
    "## Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dimension: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dimension = dimension\n",
    "\n",
    "        self.encoding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                size=(\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                ),\n",
    "                requires_grad=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            return x + self.encoding\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        embedding_dimension: int = 512,\n",
    "    ):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.total_patches = (self.image_size // self.patch_size) ** 2\n",
    "\n",
    "        if self.embedding_dimension is None:\n",
    "            warnings.warn(\n",
    "                \"Embedding dimension not specified. Using the default value calculated as: image_channels × patch_size × patch_size.\"\n",
    "            )\n",
    "            self.embedding_dimension = (\n",
    "                self.image_channels**self.patch_size * self.patch_size\n",
    "            )\n",
    "\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=self.image_channels,\n",
    "            out_channels=self.embedding_dimension,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "            padding=self.patch_size // self.patch_size,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.encoding = PositionalEncoding(dimension=self.embedding_dimension)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.projection(x)\n",
    "            x = x.view(x.size(0), x.size(-1) * x.size(-2), x.size(1))\n",
    "            x = self.encoding(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 3\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    embedding_dimension = 768\n",
    "\n",
    "    patchEmbedding = PatchEmbedding(\n",
    "        image_channels=image_channels,\n",
    "        image_size=image_size,\n",
    "        patch_size=patch_size,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((64, 3, 224, 224))\n",
    "\n",
    "    assert patchEmbedding(images).size() == (\n",
    "        64,\n",
    "        (image_size // patch_size) ** 2,\n",
    "        768,\n",
    "    ), \"Patch Embedding is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a57450",
   "metadata": {},
   "source": [
    "## Multi Head Attention Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b66be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor):\n",
    "    if (\n",
    "        not isinstance(query, torch.Tensor)\n",
    "        and isinstance(key, torch.Tensor)\n",
    "        and isinstance(value, torch.Tensor)\n",
    "    ):\n",
    "        raise TypeError(\"All inputs must be torch.Tensor\".capitalize())\n",
    "\n",
    "    key = key.transpose(-2, -1)\n",
    "    scores = torch.matmul(query, key) / math.sqrt(key.size(-1))\n",
    "    scores = torch.softmax(scores, dim=-1)\n",
    "    attention = torch.matmul(scores, value)\n",
    "    return attention\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 1\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    embedding_dimension = image_channels * patch_size * patch_size\n",
    "\n",
    "    attention = scaled_dot_product(\n",
    "        query=torch.randn(1, total_patches, embedding_dimension),\n",
    "        key=torch.randn(1, total_patches, embedding_dimension),\n",
    "        value=torch.randn(1, total_patches, embedding_dimension),\n",
    "    )\n",
    "\n",
    "    assert attention.size() == (\n",
    "        total_patches // total_patches,\n",
    "        total_patches,\n",
    "        embedding_dimension,\n",
    "    ), \"Attention output size must match the input size\".capitalize()\n",
    "\n",
    "\n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, nheads: int = 6, dimension: int = 768):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dimension = dimension\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.nheads == 0\n",
    "        ), \"Dimension must be divisible by number of heads\".capitalize()\n",
    "\n",
    "        warnings.warn(\n",
    "            \"Invalid number of dimensions provided. To avoid errors, ensure the dimension is calculated as: in_channels × patch_size × patch_size.\"\n",
    "        )\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "        else:\n",
    "            QKV = self.QKV(x)\n",
    "            query, key, value = torch.chunk(input=QKV, chunks=3, dim=-1)\n",
    "            assert (\n",
    "                query.size() == key.size() == value.size()\n",
    "            ), \"Query, key, and value must have the same size\".capitalize()\n",
    "\n",
    "            query = query.view(\n",
    "                query.size(0), query.size(1), self.nheads, query.size(-1) // self.nheads\n",
    "            )\n",
    "            key = key.view(\n",
    "                key.size(0), key.size(1), self.nheads, key.size(-1) // self.nheads\n",
    "            )\n",
    "            value = value.view(\n",
    "                value.size(0), value.size(1), self.nheads, value.size(-1) // self.nheads\n",
    "            )\n",
    "\n",
    "            query = query.permute(0, 2, 1, 3)\n",
    "            key = key.permute(0, 2, 1, 3)\n",
    "            value = value.permute(0, 2, 1, 3)\n",
    "\n",
    "            attention = scaled_dot_product(query=query, key=key, value=value)\n",
    "            attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(-2),\n",
    "                attention.size(1),\n",
    "                attention.size(-1),\n",
    "            )\n",
    "            attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(1),\n",
    "                attention.size(2) * attention.size(3),\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                attention.size() == x.size()\n",
    "            ), \"Attention output must have the same size as input\".capitalize()\n",
    "\n",
    "            return attention\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nheads = 8\n",
    "    dimension = 256\n",
    "\n",
    "    images = torch.randn((1, 196, 256))\n",
    "    multihead_attention = MultiHeadAttentionLayer(nheads=nheads, dimension=dimension)\n",
    "\n",
    "    assert (\n",
    "        multihead_attention(x=images)\n",
    "    ).size() == images.size(), \"MultiHeadAttention is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fddf55",
   "metadata": {},
   "source": [
    "## Layer Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape: int = 768, eps=1e-05):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.dimension = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(\n",
    "            data=torch.ones(\n",
    "                (\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                )\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        self.beta = nn.Parameter(\n",
    "            data=torch.zeros(\n",
    "                (\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension // self.dimension,\n",
    "                    self.dimension,\n",
    "                )\n",
    "            ),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "        else:\n",
    "            mean = torch.mean(x, dim=-1)\n",
    "            variance = torch.var(x, dim=-1)\n",
    "\n",
    "            mean = mean.unsqueeze(-1)\n",
    "            variance = variance.unsqueeze(-1)\n",
    "\n",
    "            x_bar = (x - mean) / torch.sqrt(variance + self.eps)\n",
    "\n",
    "            return self.alpha * x_bar + self.beta\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_channels = 3\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    \n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    dimension = (image_channels * patch_size * patch_size)\n",
    "    \n",
    "    norm = LayerNormalization(normalized_shape=dimension)\n",
    "    images = torch.randn((image_channels//image_channels, total_patches, dimension))\n",
    "\n",
    "    assert (\n",
    "        norm(images).size()\n",
    "    ) == images.size(), \"Layer Normalization is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4149f",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0db42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "    ):\n",
    "        super(FeedForwardNeuralNetwork, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation_func = activation\n",
    "\n",
    "        self.in_features = self.d_model\n",
    "        self.out_features = self.dim_feedforward\n",
    "\n",
    "        if self.activation_func == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif self.activation_func == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        elif self.activation_func == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\".capitalize())\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers += [\n",
    "                nn.Linear(\n",
    "                    in_features=self.in_features,\n",
    "                    out_features=self.out_features,\n",
    "                    bias=False,\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers += [self.activation]\n",
    "                self.layers += [nn.Dropout(p=self.dropout)]\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = self.d_model\n",
    "\n",
    "        self.network = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.network(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_size = 224\n",
    "    patch_size = 16\n",
    "    image_channels = 1\n",
    "    dropout = 0.1\n",
    "\n",
    "    total_patches = (image_size // patch_size) ** 2\n",
    "    dimension = image_channels * patch_size**2\n",
    "    dim_feedforward = 4 * dimension\n",
    "\n",
    "    images = torch.randn(image_channels // image_channels, total_patches, dimension)\n",
    "\n",
    "    network = FeedForwardNeuralNetwork(\n",
    "        d_model=dimension,\n",
    "        dim_feedforward=dim_feedforward,\n",
    "        dropout=dropout,\n",
    "    )\n",
    "    assert (\n",
    "        network(images).size()\n",
    "    ) == images.size(), \"FFNN is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9628ee10",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d74932e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        nhead: int = 8,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.nheads = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttentionLayer(\n",
    "            nheads=self.nheads,\n",
    "            dimension=self.d_model,\n",
    "        )\n",
    "\n",
    "        self.layer_norm1 = LayerNormalization(\n",
    "            normalized_shape=self.d_model, eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.layer_norm2 = LayerNormalization(\n",
    "            normalized_shape=self.d_model, eps=self.layer_norm_eps\n",
    "        )\n",
    "        self.feed_forward_network = FeedForwardNeuralNetwork(\n",
    "            d_model=self.d_model,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(p=self.dropout)\n",
    "        self.dropout2 = nn.Dropout(p=self.dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            residual = x\n",
    "            \n",
    "            x = self.multi_head_attention(x)\n",
    "            x = self.dropout1(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm1(x)\n",
    "            \n",
    "            residual = x\n",
    "            \n",
    "            x = self.feed_forward_network(x)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm2(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    transformer = TransformerEncoderBlock(\n",
    "        nhead=8,\n",
    "        d_model=256,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "    \n",
    "    images = torch.randn((1, 196, 256))\n",
    "    \n",
    "    assert (transformer(images).size()) == images.size(), \"Transformer is not working properly\".capitalize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73328283",
   "metadata": {},
   "source": [
    "## ViT - With Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dd190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, dimension: int = 768, dropout: float = 0.3, activation: str = \"leaky\"\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.dropout = dropout\n",
    "        self.activation_func = activation\n",
    "\n",
    "        if self.activation_func == \"relu\":\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif self.activation_func == \"leaky\":\n",
    "            self.activation = nn.LeakyReLU(inplace=True)\n",
    "        elif self.activation_func == \"gelu\":\n",
    "            self.activation = nn.GELU()\n",
    "        elif self.activation_func == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid activation function\")\n",
    "\n",
    "        self.in_features = self.dimension\n",
    "        self.out_features = self.in_features // 4\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers += [\n",
    "                nn.Linear(in_features=self.in_features, out_features=self.out_features)\n",
    "            ]\n",
    "\n",
    "            if index == 0:\n",
    "                self.layers += [nn.BatchNorm1d(num_features=self.out_features)]\n",
    "                self.layers += [self.activation]\n",
    "                self.layers += [nn.Dropout(p=self.dropout)]\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = self.out_features // 4\n",
    "\n",
    "        self.layers += [\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=self.in_features, out_features=4),\n",
    "                nn.Softmax(dim=1),\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        self.classifier = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            return self.classifier(x)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = Classifier(\n",
    "        dimension=256,\n",
    "        dropout=0.3,\n",
    "        activation=\"leaky\",\n",
    "    )\n",
    "\n",
    "    images = torch.randn((16, 196, 256))\n",
    "    images = torch.mean(images, dim=1)\n",
    "    \n",
    "    assert (classifier(images).size()) == (16, 4), \"Classifier is not working properly\".capitalize()\n",
    "\n",
    "class ViTWithClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        target_size: int = 4,\n",
    "        encoder_layer: int = 4,\n",
    "        nhead: int = 8,\n",
    "        d_model: int = 768,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"gelu\",\n",
    "        layer_norm_eps: float = 1e-05,\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super(ViTWithClassifier, self).__init__()\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.target_size = target_size\n",
    "        self.encoder_layer = encoder_layer\n",
    "        self.nhead = nhead\n",
    "        self.d_model = d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.activation = activation\n",
    "        self.layer_norm_eps = layer_norm_eps\n",
    "        self.bias = bias\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_channels=self.image_channels,\n",
    "            image_size=self.image_size,\n",
    "            patch_size=self.patch_size,\n",
    "            embedding_dimension=self.d_model,\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[\n",
    "                TransformerEncoderBlock(\n",
    "                    nhead=self.nhead,\n",
    "                    d_model=self.d_model,\n",
    "                    dim_feedforward=self.dim_feedforward,\n",
    "                    dropout=self.dropout,\n",
    "                    activation=self.activation,\n",
    "                    layer_norm_eps=self.layer_norm_eps,\n",
    "                    bias=self.bias,\n",
    "                )\n",
    "                for _ in tqdm(\n",
    "                    range(self.encoder_layer), desc=\"Transformer Block\".title()\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.classifier = Classifier(\n",
    "            dimension=self.d_model,\n",
    "            dropout=self.dropout,\n",
    "            activation=self.activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            raise TypeError(\"Input must be a torch.Tensor\")\n",
    "        else:\n",
    "            x = self.patch_embedding(x)\n",
    "\n",
    "            for layer in self.transformer:\n",
    "                x = layer(x)\n",
    "\n",
    "            x = torch.mean(x, dim=1)\n",
    "            x = self.classifier(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vit = ViTWithClassifier(\n",
    "        image_channels=1,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        target_size=4,\n",
    "        encoder_layer=4,\n",
    "        nhead=8,\n",
    "        d_model=256,\n",
    "        dim_feedforward=4 * 256,\n",
    "        dropout=0.1,\n",
    "        activation=\"gelu\",\n",
    "        layer_norm_eps=1e-05,\n",
    "        bias=False,\n",
    "    )\n",
    "\n",
    "    images = torch.randn((16, 1, 224, 224))\n",
    "\n",
    "    assert (vit(images).size()) == (16, 4), \"ViTWithClassifier is not working properly\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4188cdf7",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Criterion(nn.Module):\n",
    "    def __init__(self, loss_function: str = \"cross_entropy\", reduction: str = \"mean\"):\n",
    "        super(Criterion, self).__init__()\n",
    "        self.loss_function = loss_function\n",
    "        self.reduction = reduction\n",
    "\n",
    "        if loss_function == \"cross_entropy\":\n",
    "            self.criterion = nn.CrossEntropyLoss(reduction=self.reduction)\n",
    "        elif loss_function == \"cross_entropy_with_logits\":\n",
    "            self.criterion = nn.CrossEntropyLossWithLogits(reduction=self.reduction)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function\")\n",
    "\n",
    "    def forward(self, actual: torch.Tensor, predicted: torch.Tensor):\n",
    "        if not isinstance(actual, torch.Tensor) or not isinstance(\n",
    "            predicted, torch.Tensor\n",
    "        ):\n",
    "            raise ValueError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "        return self.criterion(actual, predicted)\n",
    "\n",
    "\n",
    "def load_dataloader():\n",
    "    dataloader_path = \"./data/processed\"\n",
    "    train_dataloader = os.path.join(dataloader_path, \"train_dataloader.pkl\")\n",
    "    test_dataloader = os.path.join(dataloader_path, \"test_dataloader.pkl\")\n",
    "    valid_dataloader = os.path.join(dataloader_path, \"valid_dataloader.pkl\")\n",
    "\n",
    "    train_dataloader = load_files(filename=train_dataloader)\n",
    "    test_dataloader = load_files(filename=test_dataloader)\n",
    "    valid_dataloader = load_files(filename=valid_dataloader)\n",
    "\n",
    "    return {\n",
    "        \"train_dataloader\": train_dataloader,\n",
    "        \"test_dataloader\": test_dataloader,\n",
    "        \"valid_dataloader\": valid_dataloader,\n",
    "    }\n",
    "\n",
    "\n",
    "def helper(**kwargs):\n",
    "    model = kwargs[\"model\"]\n",
    "    lr: float = kwargs[\"lr\"]\n",
    "    weight_decay: float = kwargs[\"weight_decay\"]\n",
    "    adam: bool = kwargs[\"adam\"]\n",
    "    beta1: float = kwargs[\"beta1\"]\n",
    "    beta2: float = kwargs[\"beta2\"]\n",
    "    SGD: bool = kwargs[\"SGD\"]\n",
    "    momentum: float = kwargs[\"momentum\"]\n",
    "\n",
    "    if model is None:\n",
    "        classifier = ViTWithClassifier(\n",
    "            image_channels=1,\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            target_size=4,\n",
    "            encoder_layer=4,\n",
    "            nhead=8,\n",
    "            d_model=256,\n",
    "            dim_feedforward=2048,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\",\n",
    "            layer_norm_eps=1e-05,\n",
    "            bias=False,\n",
    "        )\n",
    "    else:\n",
    "        classifier = model\n",
    "\n",
    "    if adam:\n",
    "        optimizer = optim.Adam(\n",
    "            params=classifier.parameters(),\n",
    "            lr=lr,\n",
    "            betas=(beta1, beta2),\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    elif SGD:\n",
    "        optimizer = optim.SGD(\n",
    "            params=classifier.parameters(),\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Optimizer not found, use 'adam' or 'SGD'\")\n",
    "    try:\n",
    "        dataloader = load_dataloader()\n",
    "    except Exception as e:\n",
    "        raise ValueError(\n",
    "            \"Dataloader not found, use 'load_dataloader' to load dataloader\"\n",
    "        )\n",
    "\n",
    "    try:\n",
    "        criterion = Criterion(loss_function=\"cross_entropy\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Criterion not found, use 'Criterion' to load criterion\")\n",
    "\n",
    "    return {\n",
    "        \"classifier\": classifier,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"criterion\": criterion,\n",
    "        \"dataloader\": dataloader,\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    init = helper(\n",
    "        model=None,\n",
    "        lr=1e-4,\n",
    "        adam=True,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        weight_decay=0.01,\n",
    "        SGD=False,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "\n",
    "    train_dataloader = init[\"dataloader\"][\"train_dataloader\"]\n",
    "    test_dataloader = init[\"dataloader\"][\"test_dataloader\"]\n",
    "    valid_dataloader = init[\"dataloader\"][\"valid_dataloader\"]\n",
    "\n",
    "    classifier = init[\"classifier\"]\n",
    "\n",
    "    criterion = init[\"criterion\"]\n",
    "\n",
    "    assert train_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "    assert test_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "    assert valid_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "\n",
    "    assert classifier.__class__ == ViTWithClassifier\n",
    "    assert criterion.__class__ == Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e366c612",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad497006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,\n",
    "        epochs: int = 100,\n",
    "        lr: float = 0.001,\n",
    "        beta1: float = 0.5,\n",
    "        beta2: float = 0.999,\n",
    "        weight_decay: float = 0.0,\n",
    "        momentum: float = 0.85,\n",
    "        adam: bool = True,\n",
    "        SGD: bool = False,\n",
    "        l1_regularization: bool = False,\n",
    "        elasticNet_regularization: bool = False,\n",
    "        device: str = \"cuda\",\n",
    "        verbose: bool = True,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.adam = adam\n",
    "        self.SGD = SGD\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.elasticNet_regularization = elasticNet_regularization\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.init = helper(\n",
    "            model=self.model,\n",
    "            lr=self.lr,\n",
    "            adam=self.adam,\n",
    "            beta1=self.beta1,\n",
    "            beta2=self.beta2,\n",
    "            weight_decay=self.weight_decay,\n",
    "            SGD=self.SGD,\n",
    "            momentum=self.momentum,\n",
    "        )\n",
    "\n",
    "        self.device = device_init(device=device)\n",
    "\n",
    "        self.train_dataloader = self.init[\"dataloader\"][\"train_dataloader\"]\n",
    "        self.test_dataloader = self.init[\"dataloader\"][\"test_dataloader\"]\n",
    "        self.valid_dataloader = self.init[\"dataloader\"][\"valid_dataloader\"]\n",
    "\n",
    "        self.optimizer = self.init[\"optimizer\"]\n",
    "        self.criterion = self.init[\"criterion\"]\n",
    "        self.classifier = self.init[\"classifier\"]\n",
    "\n",
    "        self.classifier = self.classifier.to(self.device)\n",
    "        self.criterion = self.criterion.to(self.device)\n",
    "\n",
    "        assert self.train_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "        assert self.test_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "        assert self.valid_dataloader.__class__ == torch.utils.data.dataloader.DataLoader\n",
    "\n",
    "        assert self.classifier.__class__ == ViTWithClassifier\n",
    "        assert self.criterion.__class__ == Criterion\n",
    "\n",
    "        if self.adam:\n",
    "            assert self.optimizer.__class__ == torch.optim.Adam\n",
    "        elif self.SGD:\n",
    "            assert self.optimizer.__class__ == torch.optim.SGD\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer not supported\".capitalize())\n",
    "\n",
    "        self.loss = float(\"inf\")\n",
    "\n",
    "    def l1_regularizer(self, model: ViTWithClassifier):\n",
    "        if not isinstance(model, ViTWithClassifier):\n",
    "            raise ValueError(\"Model must be a ViTWithClassifier\".capitalize())\n",
    "        return 0.01 * sum(\n",
    "            torch.norm(input=params, p=1) for params in model.parameters()\n",
    "        )\n",
    "\n",
    "    def elasticNet_regularizer(self, model: ViTWithClassifier):\n",
    "        if not isinstance(model, ViTWithClassifier):\n",
    "            raise ValueError(\"Model must be a ViTWithClassifier\".capitalize())\n",
    "        return 0.01 * sum(\n",
    "            torch.norm(input=params, p=1) for params in model.parameters()\n",
    "        ) + 0.01 * sum(torch.norm(input=params, p=2) for params in model.parameters())\n",
    "\n",
    "    def saved_checkpoints(self, train_loss: float, epoch: int = 1):\n",
    "        if not isinstance(train_loss, float):\n",
    "            raise ValueError(\"Train Loss must be a tensor\".capitalize())\n",
    "\n",
    "        if train_loss < self.loss:\n",
    "            self.loss = train_loss\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"train_loss\": self.loss,\n",
    "                    \"epoch\": self.epochs,\n",
    "                    \"model_state_dict\": self.classifier.state_dict(),\n",
    "                    \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                },\n",
    "                os.path.join(\"./artifacts/checkpoints/best_model\", \"best_model.pth\"),\n",
    "            )\n",
    "\n",
    "        torch.save(\n",
    "            self.classifier.state_dict(),\n",
    "            os.path.join(\"./artifacts/checkpoints/train_models\", f\"model{epoch}.pth\"),\n",
    "        )\n",
    "\n",
    "    def update_training(self, predicted: torch.Tensor, actual: torch.Tensor):\n",
    "        if not isinstance(predicted, torch.Tensor) and isinstance(actual, torch.Tensor):\n",
    "            raise ValueError(\"Predicted and Actual must be tensors\".capitalize())\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        predicted_loss = self.criterion(predicted, actual)\n",
    "\n",
    "        if self.l1_regularization:\n",
    "            predicted_loss += self.l1_regularizer(self.classifier)\n",
    "        elif self.elasticNet_regularization:\n",
    "            predicted_loss += self.elasticNet_regularizer(self.classifier)\n",
    "\n",
    "        predicted_loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return predicted_loss.item()\n",
    "\n",
    "    def display(self, **kwargs):\n",
    "        epoch = kwargs[\"epoch\"]\n",
    "        train_loss = kwargs[\"train_loss\"]\n",
    "        valid_loss = kwargs[\"valid_loss\"]\n",
    "        train_accuracy = kwargs[\"train_accuracy\"]\n",
    "        valid_accuracy = kwargs[\"valid_accuracy\"]\n",
    "\n",
    "        print(\n",
    "            \"Epochs - [{}/{}] - train_loss: {:.4f} - test_loss: {:.4f} - train_accuracy: {:.4f} - test_accuracy: {:.4f}\".format(\n",
    "                epoch,\n",
    "                self.epochs,\n",
    "                train_loss,\n",
    "                valid_loss,\n",
    "                train_accuracy,\n",
    "                valid_accuracy,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs), desc=\"Training Medical-Assistant\"):\n",
    "            train_loss = []\n",
    "            valid_loss = []\n",
    "            total_train_predicted_labels = []\n",
    "            total_valid_predicted_labels = []\n",
    "            total_train_actual_labels = []\n",
    "            total_valid_actual_labels = []\n",
    "\n",
    "            for images, labels in self.train_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predicted = self.classifier(images)\n",
    "\n",
    "                train_loss.append(\n",
    "                    self.update_training(predicted=predicted, actual=labels)\n",
    "                )\n",
    "                predicted = torch.argmax(input=predicted, dim=1)\n",
    "                predicted = predicted.detach().cpu().numpy()\n",
    "\n",
    "                total_train_predicted_labels.append(predicted)\n",
    "                total_train_actual_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "            for images, labels in self.test_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predicted = self.classifier(images)\n",
    "\n",
    "                valid_loss.append(self.criterion(predicted, labels).item())\n",
    "\n",
    "                predicted = torch.argmax(input=predicted, dim=1)\n",
    "                predicted = predicted.detach().cpu().numpy()\n",
    "\n",
    "                total_valid_predicted_labels.append(predicted)\n",
    "                total_valid_actual_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "            train_accuracy = accuracy_score(\n",
    "                np.concatenate(total_train_predicted_labels),\n",
    "                np.concatenate(total_train_actual_labels),\n",
    "            )\n",
    "            valid_accuracy = accuracy_score(\n",
    "                np.concatenate(total_valid_predicted_labels),\n",
    "                np.concatenate(total_valid_actual_labels),\n",
    "            )\n",
    "\n",
    "            self.display(\n",
    "                epoch=epoch + 1,\n",
    "                train_loss=np.mean(train_loss),\n",
    "                valid_loss=np.mean(valid_loss),\n",
    "                train_accuracy=train_accuracy,\n",
    "                valid_accuracy=valid_accuracy,\n",
    "            )\n",
    "\n",
    "            self.saved_checkpoints(train_loss=np.mean(train_loss), epoch=epoch + 1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    trainer = Trainer(\n",
    "        model=None,\n",
    "        epochs=100,\n",
    "        lr=0.001,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        weight_decay=0.0001,\n",
    "        momentum=0.85,\n",
    "        adam=True,\n",
    "        SGD=False,\n",
    "        device=\"mps\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9ebaba",
   "metadata": {},
   "source": [
    "## Medical Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8730d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalAssistant:\n",
    "    def __init__(self, device: str = \"cuda\", image: str = None):\n",
    "        self.device = device\n",
    "        self.image = image\n",
    "\n",
    "        self.image_channels = 1\n",
    "\n",
    "        self.device = device_init(device=device)\n",
    "\n",
    "        self.classifier = ViTWithClassifier(\n",
    "            image_channels=1,\n",
    "            image_size=224,\n",
    "            patch_size=16,\n",
    "            target_size=4,\n",
    "            encoder_layer=1,\n",
    "            nhead=8,\n",
    "            d_model=256,\n",
    "            dim_feedforward=256,\n",
    "            dropout=0.1,\n",
    "            activation=\"gelu\",\n",
    "            layer_norm_eps=1e-05,\n",
    "            bias=False,\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def load_model(self):\n",
    "        path = \"./artifacts/checkpoints/best_model/best_model.pth\"\n",
    "        model = torch.load(path)\n",
    "\n",
    "        state_dict = model[\"model_state_dict\"]\n",
    "        self.classifier.load_state_dict(state_dict=state_dict)\n",
    "\n",
    "    def preprocess_image(self):\n",
    "        if self.image_channels == 1:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Grayscale(num_output_channels=1),\n",
    "                    transforms.CenterCrop((224, 224)),\n",
    "                    transforms.Normalize([0.5], [0.5]),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        image = cv2.imread(self.image)\n",
    "        image = Image.fromarray(image)\n",
    "        image = transform(image)\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "        return image.to(self.device)\n",
    "\n",
    "    def chat(self):\n",
    "        image = self.preprocess_image()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted = self.classifier(image)\n",
    "\n",
    "            score = torch.softmax(input=predicted, dim=1)\n",
    "            score, _ = torch.max(score, dim=1)\n",
    "            score = f\"{round(score.item() * 100, 2)}%\"\n",
    "\n",
    "            predicted = torch.argmax(predicted, dim=1)[0]\n",
    "\n",
    "            if predicted == 0:\n",
    "                labels = \"Brain: glioma\".title()\n",
    "            elif predicted == 1:\n",
    "                labels = \"Brain: Meningioma\".title()\n",
    "            elif predicted == 2:\n",
    "                labels = \"Brain: No Tumor\".title()\n",
    "            else:\n",
    "                labels = \"Brain: Pituitary\".title()\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        llm = ChatOpenAI()\n",
    "        parser = StrOutputParser()\n",
    "\n",
    "        initial_response = classifier_prompt | llm | parser\n",
    "        initial_response = initial_response.invoke(\n",
    "            {\"predicted_disease\": labels, \"predicted_probability\": score}\n",
    "        )\n",
    "\n",
    "        print(initial_response)\n",
    "\n",
    "        self.memory.append(\"AI Response:\\n\" + \" \" + initial_response)\n",
    "\n",
    "        while True:\n",
    "            question = input(\"Human: \")\n",
    "            if question == \"exit\":\n",
    "                break\n",
    "\n",
    "            question = \"\\n\".join(self.memory) + \"\\nHuman: \" + question\n",
    "\n",
    "            response = QA_prompt | llm | parser\n",
    "            response = response.invoke({\"question\": question})\n",
    "\n",
    "            self.memory.append(\"AI Response:\\n\" + \" \" + response)\n",
    "\n",
    "            print(\"AI:\\n\", response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Medical Assistant chatbot\".title())\n",
    "    parser.add_argument(\"--image\", type=str, help=\"Path to the image file\".capitalize())\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, help=\"Device to run the model on\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    assistant = MedicalAssistant(device=args.device, image=args.image)\n",
    "\n",
    "    assistant.load_model()\n",
    "    assistant.chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f558a0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
